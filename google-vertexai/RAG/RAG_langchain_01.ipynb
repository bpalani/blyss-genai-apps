{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bpalani/blyss-genai-apps/blob/main/google-vertexai/RAG/RAG_langchain_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tiz29MFRmnw5",
        "outputId": "4d18f1d1-6e20-4ab9-aedb-676c135e5d3d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting virtualenv\n",
            "  Downloading virtualenv-20.30.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (3.18.0)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (4.3.7)\n",
            "Downloading virtualenv-20.30.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: distlib, virtualenv\n",
            "Successfully installed distlib-0.3.9 virtualenv-20.30.0\n",
            "created virtual environment CPython3.11.12.final.0-64 in 828ms\n",
            "  creator CPython3Posix(dest=/content/myenv, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==25.0.1, setuptools==78.1.0, wheel==0.45.1\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
          ]
        }
      ],
      "source": [
        "#Setting up virtual env\n",
        "!pip install virtualenv\n",
        "!virtualenv myenv\n",
        "!source myenv/bin/activate;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Langchain, Langgraph, VertexAI and other required packages\n",
        "!pip install -q langchain langchain_core langchain_community\n",
        "!pip install -q langchain_google_genai\n",
        "!pip install -q langchain-ollama"
      ],
      "metadata": {
        "id": "oSMEC2fCndAB",
        "collapsed": true
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Authenticate using Google API key\n",
        "from google.colab import userdata\n",
        "import os\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "SG0P5CJmniLi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize GenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
        "llm.invoke(\"What is the capital of France?\")"
      ],
      "metadata": {
        "id": "JGTBLU8vsmOj",
        "outputId": "ccb22570-40a8-44dd-ecf1-4defecdf4f00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The capital of France is **Paris**.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-6ad4eaf0-d65e-4f20-9999-58dfd6d804a9-0', usage_metadata={'input_tokens': 7, 'output_tokens': 9, 'total_tokens': 16, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "import bs4\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "# Load and chunk contents of the blog\n",
        "loader = WebBaseLoader(\n",
        "    web_path=(\"https://raw.githubusercontent.com/influxdata/influxdb3_plugins/refs/heads/main/aditya-sairam/README.md\",)\n",
        "    )\n",
        "docs = loader.load()\n",
        "#print(docs[0])"
      ],
      "metadata": {
        "id": "ZP9zrmmGtJnG"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"Summarize this content: {context}. Why would I use this? How would I use this?\")\n",
        "chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "result = chain.invoke({\"context\": docs})\n",
        "result"
      ],
      "metadata": {
        "id": "vkNGddHAJ7mc",
        "outputId": "13b2963d-bcb3-49f6-9407-6cb9ae9e11e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"This document describes a Python plugin for the InfluxDB 3 processing engine that automatically calculates and stores statistical metrics for numerical columns in a table whenever new data is written. It's similar to Python's `.describe()` method but integrated directly into the database workflow.  It also details how to set up time buckets for calculating stats over intervals, and how to expose the analytics data through a Redis-backed FastAPI endpoint.\\n\\n**Why would I use this?**\\n\\n*   **Automated Data Summarization:** You want to automatically generate summary statistics (min, max, mean, median, mode, 95th percentile) for your numerical data without manually running queries or scripts every time data is added.\\n*   **Real-time Insights:** You need quick access to statistical insights about your data as it's being ingested.\\n*   **Simplified Analysis:** It provides a pre-calculated summary, saving you time and effort in data exploration and analysis.\\n*   **Monitoring and Alerting:** You can use the generated statistics to monitor data trends and set up alerts if certain metrics fall outside expected ranges.\\n*   **Time-Series Analysis:** The Time Bucket Feature allows grouping data and calculating statistical metrics based on time intervals (days, minutes, seconds).\\n*   **API Access to Metrics:** The FastAPI endpoint provides a convenient way to access the cached analytics data programmatically, making it easy to integrate with dashboards, monitoring systems, or other applications.\\n\\n**How would I use this?**\\n\\n1.  **Prerequisites:** Ensure you have InfluxDB 3 set up and a database created.\\n2.  **Create a Trigger:** Use the `influxdb3 create trigger` command to configure the plugin to run whenever data is written to a specific table.  Crucially, you need to specify the table name in the `--trigger-spec` and `--trigger-arguments`.\\n3.  **Enable the Trigger:** Use `influxdb3 enable trigger` to activate the trigger.\\n4.  **Write Data:** Write data to the specified table using `influxdb3 write`.\\n5.  **Access the Analytics:**\\n    *   The plugin automatically creates a new table (e.g., `analytics_`) containing the calculated statistics.  You can query this table to view the metrics.\\n    *   **Time Bucket Feature:**  Specify a `time_sampling` argument when creating the trigger to enable the time bucket feature and generate analytics metrics based on time intervals.\\n    *   **API Endpoint:**\\n        *   Set up a Redis instance (using Docker is recommended).\\n        *   Install the `fastapi` and `uvicorn` packages using `influxdb3 install package`.\\n        *   Run the provided Docker Compose file to start the FastAPI server.\\n        *   Access the analytics data via the REST API endpoint using a `curl` command, specifying the table and database names.\\n6. **Example Trigger command for API endpoint**\\n```bash\\ninfluxdb3 create trigger \\\\\\n  --database  \\\\\\n  --trigger-spec 'table:' \\\\\\n  --trigger-arguments 'table_name:,database_name:' \\\\\\n  --plugin-filename /stats_metrics.py stats_metrics_trigger\\n```\\n7. **Example curl command to access data via the API**\\n```bash\\n curl -X 'GET' \\\\\\n    'http://localhost:8001/analytics/{table_name}?database={database_name}' \\\\\\n    -H 'accept: application/json'\\n```\\n\\nIn essence, this plugin automates the process of generating and storing summary statistics within InfluxDB 3, making it easier to gain insights from your data and integrate those insights into other systems. The addition of the Time Bucket Feature and API endpoint further enhances its utility for time-series analysis and application integration.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(result)"
      ],
      "metadata": {
        "id": "Iwsl9ax3KL3S",
        "outputId": "5e746921-36b8-465a-dce3-04ec9ab62944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This document describes a Python plugin for the InfluxDB 3 processing engine that automatically calculates and stores statistical metrics for numerical columns in a table whenever new data is written. It's similar to Python's `.describe()` method but integrated directly into the database workflow.  It also details how to set up time buckets for calculating stats over intervals, and how to expose the analytics data through a Redis-backed FastAPI endpoint.\n\n**Why would I use this?**\n\n*   **Automated Data Summarization:** You want to automatically generate summary statistics (min, max, mean, median, mode, 95th percentile) for your numerical data without manually running queries or scripts every time data is added.\n*   **Real-time Insights:** You need quick access to statistical insights about your data as it's being ingested.\n*   **Simplified Analysis:** It provides a pre-calculated summary, saving you time and effort in data exploration and analysis.\n*   **Monitoring and Alerting:** You can use the generated statistics to monitor data trends and set up alerts if certain metrics fall outside expected ranges.\n*   **Time-Series Analysis:** The Time Bucket Feature allows grouping data and calculating statistical metrics based on time intervals (days, minutes, seconds).\n*   **API Access to Metrics:** The FastAPI endpoint provides a convenient way to access the cached analytics data programmatically, making it easy to integrate with dashboards, monitoring systems, or other applications.\n\n**How would I use this?**\n\n1.  **Prerequisites:** Ensure you have InfluxDB 3 set up and a database created.\n2.  **Create a Trigger:** Use the `influxdb3 create trigger` command to configure the plugin to run whenever data is written to a specific table.  Crucially, you need to specify the table name in the `--trigger-spec` and `--trigger-arguments`.\n3.  **Enable the Trigger:** Use `influxdb3 enable trigger` to activate the trigger.\n4.  **Write Data:** Write data to the specified table using `influxdb3 write`.\n5.  **Access the Analytics:**\n    *   The plugin automatically creates a new table (e.g., `analytics_`) containing the calculated statistics.  You can query this table to view the metrics.\n    *   **Time Bucket Feature:**  Specify a `time_sampling` argument when creating the trigger to enable the time bucket feature and generate analytics metrics based on time intervals.\n    *   **API Endpoint:**\n        *   Set up a Redis instance (using Docker is recommended).\n        *   Install the `fastapi` and `uvicorn` packages using `influxdb3 install package`.\n        *   Run the provided Docker Compose file to start the FastAPI server.\n        *   Access the analytics data via the REST API endpoint using a `curl` command, specifying the table and database names.\n6. **Example Trigger command for API endpoint**\n```bash\ninfluxdb3 create trigger \\\n  --database  \\\n  --trigger-spec 'table:' \\\n  --trigger-arguments 'table_name:,database_name:' \\\n  --plugin-filename /stats_metrics.py stats_metrics_trigger\n```\n7. **Example curl command to access data via the API**\n```bash\n curl -X 'GET' \\\n    'http://localhost:8001/analytics/{table_name}?database={database_name}' \\\n    -H 'accept: application/json'\n```\n\nIn essence, this plugin automates the process of generating and storing summary statistics within InfluxDB 3, making it easier to gain insights from your data and integrate those insights into other systems. The addition of the Time Bucket Feature and API endpoint further enhances its utility for time-series analysis and application integration."
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    }
  ]
}